{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "9ChhSdKH8fo_"
      },
      "id": "9ChhSdKH8fo_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip freeze | grep cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW24AaxdPPL8",
        "outputId": "d538241c-acfd-45ea-81e9-e18960072561"
      },
      "id": "rW24AaxdPPL8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cupy-cuda11x==11.0.0\n",
            "jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.25+cuda11.cudnn805-cp37-cp37m-manylinux2014_x86_64.whl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qZFqTrBK89fU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b12cd87-0545-4aed-b8a8-d4ddf19b5489"
      },
      "id": "qZFqTrBK89fU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "VO5rZup7zOSa"
      },
      "id": "VO5rZup7zOSa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_default = '/content/drive/MyDrive/2022-2/TCC2/DESENVOLVIMENTO/plate_detection'"
      ],
      "metadata": {
        "id": "1_yfWx3P9Z3G"
      },
      "id": "1_yfWx3P9Z3G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "792978b8",
      "metadata": {
        "id": "792978b8"
      },
      "source": [
        "# Setup Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59800bfb",
      "metadata": {
        "id": "59800bfb"
      },
      "outputs": [],
      "source": [
        "CUSTOM_MODEL_NAME = 'my_ssd_mobnet_6'\n",
        "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
        "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
        "DATASETS_URL = 'https://www.kaggle.com/andrewmvd/car-plate-detection/download'\n",
        "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
        "LABEL_MAP_NAME = 'label_map.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dac9dba",
      "metadata": {
        "id": "8dac9dba"
      },
      "outputs": [],
      "source": [
        "paths = {\n",
        "    'WORKSPACE_PATH': os.path.join(path_default, 'Tensorflow', 'workspace'),\n",
        "    'SCRIPTS_PATH': os.path.join(path_default, 'Tensorflow','scripts'),\n",
        "    'APIMODEL_PATH': os.path.join(path_default, 'Tensorflow','models'),\n",
        "    'ANNOTATION_PATH': os.path.join(path_default, 'Tensorflow', 'workspace','annotations'),\n",
        "    'IMAGE_PATH': os.path.join(path_default, 'Tensorflow', 'workspace','images'),\n",
        "    'IMAGE_PATH_TRAIN': os.path.join(path_default, 'Tensorflow', 'workspace','images','train'),\n",
        "    'IMAGE_PATH_TEST': os.path.join(path_default, 'Tensorflow', 'workspace','images','test'),\n",
        "    'IMAGE_PATH_EVAL': os.path.join(path_default, 'Tensorflow', 'workspace','images','eval'),\n",
        "    'IMAGE_PATH_TFLITE': os.path.join(path_default, 'Tensorflow', 'workspace','images','tflite_image'),\n",
        "    'MODEL_PATH': os.path.join(path_default, 'Tensorflow', 'workspace','models'),\n",
        "    'PRETRAINED_MODEL_PATH': os.path.join(path_default, 'Tensorflow', 'workspace','pre-trained-models'),\n",
        "    'CHECKPOINT_PATH': os.path.join(path_default, 'Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME,'checkpoint'), \n",
        "    'OUTPUT_PATH': os.path.join(path_default, 'Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
        "    'TFLITE_PATH':os.path.join(path_default, 'Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
        "    'PROTOC_PATH':os.path.join(path_default, 'Tensorflow','protoc')\n",
        "    \n",
        " }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40561e7a",
      "metadata": {
        "id": "40561e7a"
      },
      "outputs": [],
      "source": [
        "files = {\n",
        "    'PIPELINE_CONFIG':os.path.join(path_default, 'Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME,'checkpoint','pipeline.config'),\n",
        "    'TF_RECORD_SCRIPT': os.path.join(path_default, paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
        "    'LABELMAP': os.path.join(path_default, paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4872ed94",
      "metadata": {
        "id": "4872ed94"
      },
      "outputs": [],
      "source": [
        "for path in paths.values():\n",
        "    if not os.path.exists(path):\n",
        "        !mkdir -p {path}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2304331a",
      "metadata": {
        "id": "2304331a"
      },
      "source": [
        "# Download Modelo pré-treinado do Tensorflow Model Zoo e API Object detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46f65c6",
      "metadata": {
        "id": "b46f65c6"
      },
      "outputs": [],
      "source": [
        "# Baixar modelo, somente na primeira execução\n",
        "if False:\n",
        "  !wget {PRETRAINED_MODEL_URL}\n",
        "  !mv {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
        "  !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccd7f7a4",
      "metadata": {
        "id": "ccd7f7a4"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection')):\n",
        "    !git clone https://github.com/tensorflow/models.git {paths['APIMODEL_PATH']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e050093",
      "metadata": {
        "collapsed": true,
        "id": "0e050093",
        "outputId": "9af33c21-7ba9-4bcd-b12e-42d27f607961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/drive/MyDrive/2022-2/TCC2/DESENVOLVIMENTO/plate_detection/Tensorflow/models/research\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.42.0-cp37-cp37m-manylinux2010_x86_64.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.32)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 68.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.6)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.3.5)\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.7.0%2Bzzzcolab20220506150900-cp37-cp37m-linux_x86_64.whl (665.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 665.5 MB 24 kB/s \n",
            "\u001b[?25hCollecting tf-models-official==2.7.0\n",
            "  Downloading tf_models_official-2.7.0-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 48.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow_io==0.23.1\n",
            "  Downloading tensorflow_io-0.23.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.1 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.9.0)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (14.0.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (1.50.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (4.1.1)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (0.27.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (0.38.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (3.3.0)\n",
            "Collecting keras\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (1.12)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (1.14.1)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (2.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->object-detection==0.1) (1.21.6)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.23.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 56.2 MB/s \n",
            "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.7.0->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.7.0->object-detection==0.1) (4.1.3)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 66.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.7.0->object-detection==0.1) (1.12.11)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.7.0->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.7.0->object-detection==0.1) (4.6.0.66)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.7.0->object-detection==0.1) (6.0)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.7.0->object-detection==0.1) (4.6.0)\n",
            "Collecting tensorflow-text>=2.7.0\n",
            "  Downloading tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 58.6 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 62.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[K     |████████████████████████████████| 238 kB 72.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.7.0->object-detection==0.1) (1.5.12)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.7.0->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (1.35.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (1.31.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (2022.6)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (1.56.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0->object-detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.7.0->object-detection==0.1) (2022.9.24)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.7.0->object-detection==0.1) (6.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.7.0->object-detection==0.1) (4.64.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.7.0->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.7.0->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->object-detection==0.1) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->object-detection==0.1) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->object-detection==0.1) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->object-detection==0.1) (3.10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.7.0->object-detection==0.1) (0.1.7)\n",
            "Collecting tensorflow-text>=2.7.0\n",
            "  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 45.6 MB/s \n",
            "\u001b[?25h  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 48.8 MB/s \n",
            "\u001b[?25h  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 58.0 MB/s \n",
            "\u001b[?25h  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (6.0.1)\n",
            "Collecting cloudpickle~=2.1.0\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.13.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (506 kB)\n",
            "\u001b[K     |████████████████████████████████| 506 kB 67.0 MB/s \n",
            "\u001b[?25hCollecting orjson<4.0\n",
            "  Downloading orjson-3.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (2022.6.2)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 62.9 MB/s \n",
            "\u001b[?25hCollecting requests<3.0.0dev,>=2.18.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting zstandard<1,>=0.18.0\n",
            "  Downloading zstandard-0.19.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 49.2 MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Collecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.7.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 50.3 MB/s \n",
            "\u001b[?25hCollecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting protobuf>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.7.0->object-detection==0.1) (2.1.1)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.6.0.66)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.7.0->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official==2.7.0->object-detection==0.1) (0.8.10)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official==2.7.0->object-detection==0.1) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.7.0->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.7.0->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official==2.7.0->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official==2.7.0->object-detection==0.1) (1.10.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official==2.7.0->object-detection==0.1) (5.10.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official==2.7.0->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official==2.7.0->object-detection==0.1) (0.9.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official==2.7.0->object-detection==0.1) (2.3)\n",
            "Building wheels for collected packages: object-detection, dill, avro-python3, docopt, seqeval\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1695174 sha256=34574e5a6fd3dd4b6257440e943538cab7b2fb1274f9cf4c67a83eb8bdabfc89\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-eq3n7jxy/wheels/56/3c/d4/bb074aa2f0ad2cf5df921c0980f0b7bd1eff7c9dd91846929d\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78545 sha256=a5e82f347a380f92e143f739c93e0cdd5c87b9943456d5bbc541747e71b8fd55\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44009 sha256=0b7d799f432bd060bc90a2d9843e503adc911c9f63ab302414a2a139f988c445\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/e5/b1/6b151d9b535ee50aaa6ab27d145a0104b6df02e5636f0376da\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=d97479cc072b4009de37e49e8b95ce286bd81b89d4fb7af708cd56c1658a5212\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16182 sha256=df851b5dd4008f0aa7fe0ce36d881134b656661fd912a9248aabaff9c81ef838\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built object-detection dill avro-python3 docopt seqeval\n",
            "Installing collected packages: requests, pyparsing, protobuf, tensorflow-io-gcs-filesystem, tensorflow-estimator, keras, tensorflow, portalocker, docopt, dill, colorama, zstandard, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pymongo, py-cpuinfo, proto-plus, orjson, hdfs, fastavro, cloudpickle, tf-models-official, tensorflow-io, lvis, avro-python3, apache-beam, object-detection\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.27.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.27.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.27.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.3.2\n",
            "    Uninstalling pymongo-4.3.2:\n",
            "      Successfully uninstalled pymongo-4.3.2\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.5.0\n",
            "    Uninstalling cloudpickle-1.5.0:\n",
            "      Successfully uninstalled cloudpickle-1.5.0\n",
            "Successfully installed apache-beam-2.42.0 avro-python3-1.10.2 cloudpickle-2.1.0 colorama-0.4.6 dill-0.3.1.1 docopt-0.6.2 fastavro-1.7.0 hdfs-2.7.0 keras-2.7.0 lvis-0.5.3 object-detection-0.1 orjson-3.8.1 portalocker-2.6.0 proto-plus-1.22.1 protobuf-3.19.6 py-cpuinfo-9.0.0 pymongo-3.13.0 pyparsing-2.4.7 requests-2.28.1 sacrebleu-2.3.1 sentencepiece-0.1.97 seqeval-1.2.2 tensorflow-2.7.0+zzzcolab20220506150900 tensorflow-addons-0.18.0 tensorflow-estimator-2.7.0 tensorflow-io-0.23.1 tensorflow-io-gcs-filesystem-0.23.1 tensorflow-model-optimization-0.7.3 tensorflow-text-2.7.3 tf-models-official-2.7.0 tf-slim-1.1.0 zstandard-0.19.0\n"
          ]
        }
      ],
      "source": [
        "# Install Tensorflow Object Detection \n",
        "\n",
        "# !sudo apt-get install protobuf-compiler\n",
        "!cd {os.path.join(path_default, 'Tensorflow/models/research')} && \\\n",
        "  protoc object_detection/protos/*.proto --python_out=. && \\\n",
        "  cp object_detection/packages/tf2/setup.py . && \\\n",
        "  python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f606ff9a",
      "metadata": {
        "id": "f606ff9a"
      },
      "outputs": [],
      "source": [
        "# Verify Installation Object Detection\n",
        "import object_detection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "708d1989",
      "metadata": {
        "id": "708d1989"
      },
      "source": [
        "# Instalando tensorflow, CUDA and cuDNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d534ad30",
      "metadata": {
        "id": "d534ad30"
      },
      "outputs": [],
      "source": [
        "# !sudo apt-get install cuda-11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49e6f902",
      "metadata": {
        "id": "49e6f902"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.5 tensorflow-gpu==2.5 --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba0b9a66",
      "metadata": {
        "id": "ba0b9a66"
      },
      "outputs": [],
      "source": [
        "VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
        "# Verify Installation\n",
        "!python {VERIFICATION_SCRIPT}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cde3992",
      "metadata": {
        "id": "8cde3992"
      },
      "source": [
        "# Conjunto de dados, mapeamento de rótulos  e TF records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28c021f1",
      "metadata": {
        "id": "28c021f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67656aa2-d647-4753-c80e-5a3e5b9ca19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fazer o download do link https://www.kaggle.com/andrewmvd/car-plate-detection/download\n",
            "\n",
            "Extrair as imagens e rotulos, dividindo-os em 95% para treino e 5% teste, movendo-as para seus caminhos:\n",
            "\n",
            "/content/drive/MyDrive/2022-2/TCC2/DESENVOLVIMENTO/plate_detection/Tensorflow/workspace/images/train\n",
            "/content/drive/MyDrive/2022-2/TCC2/DESENVOLVIMENTO/plate_detection/Tensorflow/workspace/images/test\n"
          ]
        }
      ],
      "source": [
        "print(\"Fazer o download do link \" + DATASETS_URL)\n",
        "\n",
        "print(\"\\nExtrair as imagens e rotulos, dividindo-os em 95% para treino e 5% teste, movendo-as para seus caminhos:\\n\")\n",
        "print(os.path.join(paths['IMAGE_PATH_TRAIN']))\n",
        "print(os.path.join(paths['IMAGE_PATH_TEST']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files['LABELMAP']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n3A4Lv-XJFJS",
        "outputId": "99c561b5-a48c-4217-a8a7-310c7f73af00"
      },
      "id": "n3A4Lv-XJFJS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/2022-2/TCC2/DESENVOLVIMENTO/plate_detection/Tensorflow/workspace/annotations/label_map.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77ddcc9",
      "metadata": {
        "id": "b77ddcc9"
      },
      "outputs": [],
      "source": [
        "labels = [{'name':'licence', 'id':1}]\n",
        "\n",
        "with open(files['LABELMAP'], 'w') as f:\n",
        "    for label in labels:\n",
        "        f.write('item { \\n')\n",
        "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
        "        f.write('\\tid:{}\\n'.format(label['id']))\n",
        "        f.write('}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36bb7c3c",
      "metadata": {
        "id": "36bb7c3c"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(files['TF_RECORD_SCRIPT']):\n",
        "    !git clone https://github.com/nicknochnack/GenerateTFRecord {paths['SCRIPTS_PATH']}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files['TF_RECORD_SCRIPT']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hbnDMT5mJUJJ",
        "outputId": "a6f1e1d0-9885-49a0-d90d-67c1d894d120"
      },
      "id": "hbnDMT5mJUJJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/2022-2/TCC2/DESENVOLVIMENTO/plate_detection/Tensorflow/scripts/generate_tfrecord.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5830b728",
      "metadata": {
        "id": "5830b728"
      },
      "outputs": [],
      "source": [
        "!python {files['TF_RECORD_SCRIPT']} -x {paths['IMAGE_PATH_TRAIN']} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'train.record')} \n",
        "!python {files['TF_RECORD_SCRIPT']} -x {paths['IMAGE_PATH_TEST']} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'test.record')} "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a096993",
      "metadata": {
        "id": "5a096993"
      },
      "source": [
        "# Copiando configurações do modelo pré-treinado e atualizando o pipeline para transferencia de aprendizado"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "d8RDdJ9VYkIf"
      },
      "id": "d8RDdJ9VYkIf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ac0630",
      "metadata": {
        "id": "c0ac0630"
      },
      "outputs": [],
      "source": [
        "from object_detection.utils import config_util\n",
        "from object_detection.protos import pipeline_pb2\n",
        "from google.protobuf import text_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b93b42",
      "metadata": {
        "id": "74b93b42"
      },
      "outputs": [],
      "source": [
        "config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c381c77",
      "metadata": {
        "id": "7c381c77"
      },
      "outputs": [],
      "source": [
        "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
        "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:                                                                                                                                                                                                                     \n",
        "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
        "    text_format.Merge(proto_str, pipeline_config)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e59e05",
      "metadata": {
        "id": "29e59e05"
      },
      "outputs": [],
      "source": [
        "pipeline_config.model.ssd.num_classes = len(labels)\n",
        "pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 320\n",
        "pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 320\n",
        "pipeline_config.train_config.batch_size = 4\n",
        "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
        "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
        "pipeline_config.train_input_reader.label_map_path= files['LABELMAP']\n",
        "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n",
        "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
        "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]\n",
        "\n",
        "\n",
        "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
        "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:                                                                                                                                                                                                                     \n",
        "    f.write(config_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5380051a",
      "metadata": {
        "id": "5380051a"
      },
      "source": [
        "# Treinamento do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2953de39",
      "metadata": {
        "id": "2953de39"
      },
      "outputs": [],
      "source": [
        "#solução de problemas para o treinamento\n",
        "# !pip uninstall pycocotools -y\n",
        "# !pip install pycocotools==2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e412503f",
      "metadata": {
        "id": "e412503f"
      },
      "outputs": [],
      "source": [
        "TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19d1ede9",
      "metadata": {
        "id": "19d1ede9"
      },
      "outputs": [],
      "source": [
        "command = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=10000\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21af7657",
      "metadata": {
        "id": "21af7657"
      },
      "outputs": [],
      "source": [
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e998230b",
      "metadata": {
        "id": "e998230b"
      },
      "source": [
        "# Avaliação do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d2f0dd",
      "metadata": {
        "id": "62d2f0dd"
      },
      "outputs": [],
      "source": [
        "#% Exemplo\n",
        "#python model_main_tf2.py -- \\\n",
        "#  --model_dir=$MODEL_DIR \n",
        "#  --num_train_steps=$NUM_TRAIN_STEPS \\\n",
        "#  --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \\\n",
        "#  --pipeline_config_path=$PIPELINE_CONFIG_PATH \\\n",
        "#  --alsologtostderr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f334be7",
      "metadata": {
        "id": "9f334be7"
      },
      "outputs": [],
      "source": [
        "command = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4e3867",
      "metadata": {
        "id": "4c4e3867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7943e692-aa00-4cc8-c7a3-ab2b96c48f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python /content/drive/MyDrive/2022-2/TCC2/DESENVOLVIMENTO/plate_detection/Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=/content/drive/MyDrive/2022-2/TCC2/DESENVOLVIMENTO/plate_detection/Tensorflow/workspace/models/my_ssd_mobnet_6/checkpoint --pipeline_config_path=/content/drive/MyDrive/2022-2/TCC2/DESENVOLVIMENTO/plate_detection/Tensorflow/workspace/models/my_ssd_mobnet_6/checkpoint/pipeline.config --checkpoint_dir=/content/drive/MyDrive/2022-2/TCC2/DESENVOLVIMENTO/plate_detection/Tensorflow/workspace/models/my_ssd_mobnet_6/checkpoint\n"
          ]
        }
      ],
      "source": [
        "print(command)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e73a568",
      "metadata": {
        "id": "5e73a568"
      },
      "outputs": [],
      "source": [
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f2aaed",
      "metadata": {
        "id": "e7f2aaed"
      },
      "source": [
        "# Congelando modelo e exportando para tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a583449",
      "metadata": {
        "id": "7a583449"
      },
      "outputs": [],
      "source": [
        "FREEZE_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'exporter_main_v2.py ')\n",
        "command = \"python {} --input_type=image_tensor --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(FREEZE_SCRIPT ,files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'], paths['OUTPUT_PATH'])\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "360eeb87",
      "metadata": {
        "id": "360eeb87"
      },
      "outputs": [],
      "source": [
        "TFLITE_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'export_tflite_graph_tf2.py ')\n",
        "command = \"python {} --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(TFLITE_SCRIPT ,files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'], paths['TFLITE_PATH'])\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def representative_dataset():\n",
        "  dataset_list = tf.data.Dataset.list_files(os.path.join(paths['IMAGE_PATH_TFLITE'], 'plate/*'))\n",
        "  for i in range(len(dataset_list)):\n",
        "    image = next(iter(dataset_list))\n",
        "    image = tf.io.read_file(image)\n",
        "    image = tf.io.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [320, 320])\n",
        "    image = tf.cast(image / 255., tf.float32)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    yield [image]\n",
        "\n",
        "\n",
        "def write_model(converter, save_model, quantization):\n",
        "    tf_lite_model = converter.convert()\n",
        "    open(os.path.join(save_model, f'{quantization}.tflite'), 'wb').write(tf_lite_model)\n",
        "\n",
        "\n",
        "def convert_tflite(quantization, saved_model_dir, allow_ops_tf=False):\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "\n",
        "    if quantization == 'original':\n",
        "        write_model(converter, saved_model_dir, quantization)\n",
        "    else:\n",
        "        \n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "        # allow operations compatible with tensorflow\n",
        "        if allow_ops_tf:\n",
        "            converter.target_spec.supported_ops = [\n",
        "            tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "            tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "            ]\n",
        "\n",
        "        converter.output_arrays = [\n",
        "            'TFLite_Detection_PostProcess:0',\n",
        "            'TFLite_Detection_PostProcess:1',\n",
        "            'TFLite_Detection_PostProcess:2',\n",
        "            'TFLite_Detection_PostProcess:3'\n",
        "        ]\n",
        "\n",
        "        if quantization == 'float16':\n",
        "            converter.target_spec.supported_types = [tf.float16]\n",
        "        elif quantization == 'int8' or quantization == 'full_int8':\n",
        "            converter.input_shapes = 1,320,320,3\n",
        "            converter.representative_dataset = representative_dataset\n",
        "        if quantization == 'full_int8':\n",
        "            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "            converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "            converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "\n",
        "        write_model(converter, save_model, quantization)"
      ],
      "metadata": {
        "id": "Ly6VHBG1GMsu"
      },
      "id": "Ly6VHBG1GMsu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FROZEN_TFLITE_PATH = os.path.join(paths['TFLITE_PATH'], 'saved_model')\n",
        "TFLITE_MODEL = os.path.join(paths['TFLITE_PATH'], 'saved_model')"
      ],
      "metadata": {
        "id": "dhwNQkroGn6X"
      },
      "id": "dhwNQkroGn6X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_tflite('original', FROZEN_TFLITE_PATH)"
      ],
      "metadata": {
        "id": "EOqg6Wklikfx"
      },
      "id": "EOqg6Wklikfx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_tflite('dynamic_range_quantization', FROZEN_TFLITE_PATH)"
      ],
      "metadata": {
        "id": "bAlH7WMiiiQy"
      },
      "id": "bAlH7WMiiiQy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_tflite('float16', FROZEN_TFLITE_PATH)"
      ],
      "metadata": {
        "id": "we-zIWpfc4Az"
      },
      "id": "we-zIWpfc4Az",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_tflite('int8', FROZEN_TFLITE_PATH)"
      ],
      "metadata": {
        "id": "gGFnopvqc95S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d123a6-616a-4283-b065-57848f36f0bf"
      },
      "id": "gGFnopvqc95S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 433 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convert_tflite('full_int8', FROZEN_TFLITE_PATH)"
      ],
      "metadata": {
        "id": "Mm6AtAPUe5Ve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a4225d-2e3b-43d0-8088-4d695da4a755"
      },
      "id": "Mm6AtAPUe5Ve",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 433 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = tf.saved_model.load(os.path.join(paths['TFLITE_PATH'], 'saved_model'))\n",
        "# assinaturas = model.signatures['serving_default']\n",
        "# # assinaturas.structured_outputs"
      ],
      "metadata": {
        "id": "RR-i1s3zcg5Y"
      },
      "id": "RR-i1s3zcg5Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo de inferência"
      ],
      "metadata": {
        "id": "oPG4Qzpo84Ca"
      },
      "id": "oPG4Qzpo84Ca"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# The function `get_tensor()` returns a copy of the tensor data.\n",
        "# Use `tensor()` in order to get a pointer to the tensor.\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)"
      ],
      "metadata": {
        "id": "cb0YD3aChs6S"
      },
      "id": "cb0YD3aChs6S",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tfod",
      "language": "python",
      "name": "tfod"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "708d1989"
      ],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}